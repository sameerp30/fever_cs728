{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/raid/nlp/sameer/fever/tfidf_pred_level_1_dev.json') as json_file:\n",
    "    retrieved_docs = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(retrieved_docs.items())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/nlp/sameer/anaconda3/envs/myenv/lib/python3.10/site-packages/datasets/load.py:1461: FutureWarning: The repository for fever contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/fever\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "fever_wiki = load_dataset(\"fever\", 'wiki_pages')\n",
    "# fever_nli = load_dataset(\"fever\", 'v1.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5416537/5416537 [02:41<00:00, 33642.43it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "wiki_id2lines = {}\n",
    "for dump in tqdm(fever_wiki['wikipedia_pages']):\n",
    "    if len(dump['text']) != 0:\n",
    "        wiki_id2lines[dump['id']] = [dump['text'], dump['lines']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('/raid/nlp/sameer/fever/data/fever-data/dev.jsonl') as f:\n",
    "    dev_data = [json.loads(line) for line in f]\n",
    "\n",
    "with open('/raid/nlp/sameer/fever/data/fever-data/shared_task_test.jsonl') as f:\n",
    "    test_data = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('/raid/nlp/sameer/fever/data/fever-data/train.jsonl') as f:\n",
    "    fever_nli = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 91198,\n",
       " 'verifiable': 'NOT VERIFIABLE',\n",
       " 'label': 'NOT ENOUGH INFO',\n",
       " 'claim': 'Colin Kaepernick became a starting quarterback during the 49ers 63rd season in the National Football League.',\n",
       " 'evidence': [[[108548, None, None, None]]]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# gold_docs = []\n",
    "# id2gold_docs = {}\n",
    "# id2gold_lines = {}\n",
    "claim2evi_sent = {}\n",
    "claim2evi_id = {}\n",
    "for i in range(0,len(dev_data)):\n",
    "    if dev_data[i]['label'] == 'NOT ENOUGH INFO':\n",
    "        continue\n",
    "    gold = []\n",
    "    gold_line = []\n",
    "    claim2evi_sent[dev_data[i]['claim']] = {}\n",
    "    claim2evi_id[dev_data[i]['claim']] = {}\n",
    "    for j in range(len(dev_data[i]['evidence'])):\n",
    "        for k in range(len(dev_data[i]['evidence'][j])):\n",
    "            \n",
    "            if dev_data[i]['evidence'][j][k][2] is not None:\n",
    "                claim2evi_sent[dev_data[i]['claim']][dev_data[i]['evidence'][j][k][2]] = set()\n",
    "                claim2evi_id[dev_data[i]['claim']][dev_data[i]['evidence'][j][k][2]] = set()\n",
    "            if dev_data[i]['evidence'][j][k][3] is not None:\n",
    "                try:\n",
    "                    text = wiki_id2lines[dev_data[i]['evidence'][j][k][2]][1].split(\"\\n\")[data[i]['evidence'][j][k][3]]\n",
    "                    text = text.replace(\"\\t\", \" \").strip(\",. \")\n",
    "                    text = re.sub(r'\\s+', ' ', text)\n",
    "                    text =  \" \".join(text.split()[1:])\n",
    "                    claim2evi_sent[dev_data[i]['claim']][dev_data[i]['evidence'][j][k][2]].add(text)\n",
    "                    claim2evi_id[dev_data[i]['claim']][data[i]['evidence'][j][k][2]].add(dev_data[i]['evidence'][j][k][3])\n",
    "                except:\n",
    "                    continue\n",
    "    # if len(gold):\n",
    "    #     id2gold_docs[fever_nli[i]['claim']] = set(gold)\n",
    "    # if len(gold_line):\n",
    "    #     id2gold_lines[fever_nli[i]['claim']] = set(gold_line)\n",
    "# gold_docs = [list(set(i)) for i in gold_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "sent2ids = []\n",
    "lines_corpus = []\n",
    "corpus_len = len(fever_wiki['wikipedia_pages']['text'][1:])\n",
    "for i in tqdm(range(0,corpus_len)):\n",
    "    for line in fever_wiki['wikipedia_pages'][i]['lines'].split(\"\\n\"):\n",
    "        text = line.replace(\"\\t\",\" \").strip(\",. \")\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = \" \".join(text.split()[1:])\n",
    "        if text.isnumeric()==True or len(text)==0:\n",
    "             continue\n",
    "        lines_corpus.append(text)\n",
    "print(len(lines_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lines_corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcollections\u001b[39;00m \u001b[39mimport\u001b[39;00m OrderedDict\n\u001b[0;32m----> 3\u001b[0m lines_corpus_dup \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(OrderedDict\u001b[39m.\u001b[39mfromkeys(lines_corpus))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lines_corpus' is not defined"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "lines_corpus_dup = list(OrderedDict.fromkeys(lines_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {}\n",
    "for i in range(0,len(lines_corpus_dup)):\n",
    "    mapping[lines_corpus_dup[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"id\": list(mapping.values()), \"text\": list(mapping.keys())}\n",
    "df = pd.DataFrame(data)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.to_csv('index_wiki_sent.tsv', sep=\"\\t\",index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_all = []\n",
    "queries_len = len(fever_nli_hf[\"train\"][\"id\"])\n",
    "cnt = 0\n",
    "for i in range(0,queries_len):\n",
    "    if fever_nli_hf[\"train\"][i]['evidence_sentence_id'] == -1:\n",
    "        continue\n",
    "    else:\n",
    "        # print(i)\n",
    "        if fever_nli_hf[\"train\"][i]['claim'] not in queries_all:\n",
    "            queries_all.append(fever_nli_hf[\"train\"][i]['claim'])\n",
    "        # cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Nikolaj Coster-Waldau worked with the Fox Broadcasting Company.', 0),\n",
       " ('Roman Atwood is a content creator.', 1),\n",
       " ('History of art includes architecture, dance, sculpture, music, painting, poetry literature, theatre, narrative, film, photography and graphic arts.',\n",
       "  2),\n",
       " ('The Boston Celtics play their home games at TD Garden.', 3),\n",
       " ('The Ten Commandments is an epic film.', 4)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping_queries = {}\n",
    "ids = [i for i in range(0,len(queries_all))]\n",
    "for i in range(len(ids)):\n",
    "    mapping_queries[queries_all[i]] = ids[i]\n",
    "list(mapping_queries.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Nikolaj Coster-Waldau worked with the Fox Broadcasting Company.', {'Nikolaj_Coster-Waldau': {'He then played Detective John Amsterdam in the short-lived Fox television series New Amsterdam -LRB- 2008 -RRB- , as well as appearing as Frank Pike in the 2009 Fox television film Virtuality , originally intended as a pilot . Fox television Fox Broadcasting Company New Amsterdam New Amsterdam (TV series) Virtuality Virtuality (TV series)'}, 'Fox_Broadcasting_Company': {'The Fox Broadcasting Company -LRB- often shortened to Fox and stylized as FOX -RRB- is an American English language commercial broadcast television network that is owned by the Fox Entertainment Group subsidiary of 21st Century Fox . English English language Fox Entertainment Group Fox Entertainment Group 21st Century Fox 21st Century Fox American United States English language English language commercial commercial broadcasting broadcast terrestrial television television network television network'}})\n",
      "--------------------------------------\n",
      "('Nikolaj Coster-Waldau worked with the Fox Broadcasting Company.', {'Nikolaj_Coster-Waldau': {7}, 'Fox_Broadcasting_Company': {0}})\n"
     ]
    }
   ],
   "source": [
    "print(list(claim2evi_sent.items())[0])\n",
    "print(\"--------------------------------------\")\n",
    "print(list(claim2evi_id.items())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_queries = {}\n",
    "queries_all = list(claim2evi_id.keys())\n",
    "ids = [i for i in range(0,len(claim2evi_id))]\n",
    "for i in range(len(ids)):\n",
    "    mapping_queries[queries_all[i]] = ids[i]\n",
    "# list(mapping_queries.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"id\": list(mapping_queries.values()), \"text\": list(mapping_queries.keys())}\n",
    "df = pd.DataFrame(data)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.to_csv('queries_new_wiki.tsv', sep=\"\\t\",index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(10)\n",
    "from sentence_transformers import InputExample\n",
    "\n",
    "from random import sample \n",
    "\n",
    "import re\n",
    "\n",
    "queries = []\n",
    "positives = []\n",
    "negatives = []\n",
    "train_examples = []\n",
    "\n",
    "for claim in claim2evi_id:\n",
    "    for doc_id in claim2evi_id[claim]:\n",
    "        if doc_id not in wiki_id2lines:\n",
    "            continue\n",
    "        lines_original = wiki_id2lines[doc_id][1].split(\"\\n\")\n",
    "        lines = []\n",
    "        for line in lines_original:\n",
    "            text = line.replace(\"\\t\",\" \").strip(\",. \")\n",
    "            text = re.sub(r'\\s+', ' ', text)\n",
    "            text = \" \".join(text.split()[1:])\n",
    "            lines.append(text)\n",
    "        for pos_id in claim2evi_id[claim][doc_id]:\n",
    "            pos_line = lines[pos_id]\n",
    "            lines = [lines[j] for j in range(len(lines)) if len(lines[j])>0 and lines[j].isnumeric()==False and j not in claim2evi_id[claim][doc_id]]\n",
    "            neg = sample(lines,min(len(lines), 8))\n",
    "            for m in range(0,len(neg)):\n",
    "                train_examples.append(InputExample(texts=[claim, pos_line, neg[m]]))\n",
    "\n",
    "                # negatives.append(neg[m])\n",
    "                # positives.append(pos_line)\n",
    "                # queries.append(claim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<InputExample> label: 0, texts: Nikolaj Coster-Waldau worked with the Fox Broadcasting Company.; The Fox Broadcasting Company -LRB- often shortened to Fox and stylized as FOX -RRB- is an American English language commercial broadcast television network that is owned by the Fox Entertainment Group subsidiary of 21st Century Fox . English English language Fox Entertainment Group Fox Entertainment Group 21st Century Fox 21st Century Fox American United States English language English language commercial commercial broadcasting broadcast terrestrial television television network television network; Fox and its affiliated companies operate many entertainment channels in international markets , although these do not necessarily air the same programming as the U.S. network\n"
     ]
    }
   ],
   "source": [
    "print(train_examples[14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(10)\n",
    "from random import sample \n",
    "\n",
    "import re\n",
    "\n",
    "queries = []\n",
    "positives = []\n",
    "negatives = []\n",
    "for claim in claim2evi_id:\n",
    "    for doc_id in claim2evi_id[claim]:\n",
    "        if doc_id not in wiki_id2lines:\n",
    "            continue\n",
    "        lines_original = wiki_id2lines[doc_id][1].split(\"\\n\")\n",
    "        lines = []\n",
    "        for line in lines_original:\n",
    "            text = line.replace(\"\\t\",\" \").strip(\",. \")\n",
    "            text = re.sub(r'\\s+', ' ', text)\n",
    "            text = \" \".join(text.split()[1:])\n",
    "            lines.append(text)\n",
    "        for pos_id in claim2evi_id[claim][doc_id]:\n",
    "            pos_line = lines[pos_id]\n",
    "            lines = [lines[j] for j in range(len(lines)) if len(lines[j])>0 and lines[j].isnumeric()==False and j not in claim2evi_id[claim][doc_id]]\n",
    "            neg = sample(lines,min(len(lines), 8))\n",
    "            for m in range(0,len(neg)):\n",
    "                negatives.append(neg[m])\n",
    "                positives.append(pos_line)\n",
    "                queries.append(claim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(10)\n",
    "from random import sample \n",
    "import re\n",
    "\n",
    "queries = []\n",
    "positives = []\n",
    "negatives = []\n",
    "sents = []\n",
    "labels = []\n",
    "for claim in claim2evi_id:\n",
    "    total_lines = []\n",
    "    pos_len = 0\n",
    "    for doc_id in claim2evi_id[claim]:\n",
    "        if doc_id not in wiki_id2lines:\n",
    "            continue\n",
    "        lines_original = wiki_id2lines[doc_id][1].split(\"\\n\")\n",
    "        lines = []\n",
    "        for line in lines_original:\n",
    "            text = line.replace(\"\\t\",\" \").strip(\",. \")\n",
    "            text = re.sub(r'\\s+', ' ', text)\n",
    "            text = \" \".join(text.split()[1:])\n",
    "            lines.append(text)\n",
    "\n",
    "        for pos_id in claim2evi_id[claim][doc_id]:\n",
    "            pos_line = lines[pos_id]\n",
    "            queries.append(claim)\n",
    "            sents.append(pos_line)\n",
    "            labels.append(\"positive\")\n",
    "            \n",
    "        pos_len += len(claim2evi_id[claim][doc_id])\n",
    "\n",
    "\n",
    "        lines = [lines[j] for j in range(len(lines)) if len(lines[j])>0 and lines[j].isnumeric()==False and j not in claim2evi_id[claim][doc_id]]\n",
    "        total_lines += lines\n",
    "    if pos_len > 10:\n",
    "        neg = sample(total_lines,min(len(total_lines), 2*pos_len-pos_len))\n",
    "    else:\n",
    "        neg = sample(total_lines,min(len(total_lines), 10-pos_len))\n",
    "\n",
    "    for m in range(0,len(neg)):\n",
    "        sents.append(neg[m])\n",
    "        queries.append(claim)\n",
    "        labels.append(\"negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fox 2000 Pictures released the film Soul Food.',\n",
       " 'Fox 2000 Pictures released the film Soul Food.',\n",
       " 'Fox 2000 Pictures released the film Soul Food.',\n",
       " 'Fox 2000 Pictures released the film Soul Food.',\n",
       " 'Fox 2000 Pictures released the film Soul Food.',\n",
       " 'Fox 2000 Pictures released the film Soul Food.',\n",
       " 'Telemundo is a English-language television network.',\n",
       " 'Telemundo is a English-language television network.',\n",
       " 'Telemundo is a English-language television network.',\n",
       " 'Telemundo is a English-language television network.',\n",
       " 'Telemundo is a English-language television network.']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries[0:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Soul Food is a 1997 American comedy-drama film produced by Kenneth `` Babyface '' Edmonds , Tracey Edmonds and Robert Teitel and released by Fox 2000 Pictures . Robert Teitel Robert Teitel comedy-drama film comedy-drama film Tracey Edmonds Tracey Edmonds Fox 2000 Pictures Fox 2000 Pictures\",\n",
       " 'In 2015 , it was announced that 20th Century Fox is planning a sequel for film called More Soul Food , written by Tillman , Jr. . 20th Century Fox 20th Century Fox',\n",
       " 'Featuring an ensemble cast , the film stars Vanessa L. Williams , Vivica A. Fox , Nia Long , Michael Beach , Mekhi Phifer , Jeffrey D. Sams , Irma P. Hall , Gina Ravera and Brandon Hammond . Brandon Hammond Brandon Hammond Vanessa L. Williams Vanessa L. Williams Vivica A. Fox Vivica A. Fox Nia Long Nia Long Michael Beach Michael Beach Mekhi Phifer Mekhi Phifer Jeffrey D. Sams Jeffrey D. Sams Irma P. Hall Irma P. Hall Gina Ravera Gina Ravera ensemble cast ensemble cast',\n",
       " 'Written and directed by George Tillman , Jr. -- in his major studio debut -- the film centers on the trials of an extended African-American family , held together by longstanding family traditions which begin to fade as serious problems take center stage . Written Screenwriter directed film director African-American African-American',\n",
       " 'Tillman based the family in the film on his own and Soul Food was widely acclaimed for presenting a more positive image of African-Americans than is typically seen in Hollywood films . Hollywood Hollywood',\n",
       " 'In 2000 , Showtime premiered a one-hour television series based upon the film . Showtime Showtime (TV network) television series based upon the film Soul Food (TV series)',\n",
       " 'In addition , Telemundo operates NBC Universo , a separate channel directed towards young Hispanic audiences ; Telemundo Digital Media , which distributes original programming content across mass media , the Telemundo and NBC Universo websites ; Puerto Rico telestation WKAQ-TV ; and international distribution arm Telemundo Internacional . Puerto Rico Puerto Rico NBC NBC NBC Universo NBC Universo mass media mass media telestation Television station WKAQ-TV WKAQ-TV',\n",
       " 'Hispanic Americans and Latino Americans -LRB- hispanos -LSB- isˈpanos -RSB- -RRB- are American descendants from Spain and the Spanish speaking countries of Latin America . Spanish Spain American Americans Spain Spain Latin America Latin America Hispanic Hispanic Latino Latino (demonym)',\n",
       " 'More generally , it includes all persons in the United States who self-identify as Hispanic or Latino , whether of full or partial ancestry . Hispanic Hispanic Latino Latino (demonym)',\n",
       " 'Most Hispanic Americans are of Mexican , Puerto Rican , Cuban , Salvadoran , Dominican , Guatemalan , or Colombian origin . Hispanic Hispanic Puerto Rican Puerto Ricans Mexican Mexican people Dominican People of the Dominican Republic Salvadoran Salvadoran Cuban Cubans Guatemalan Demographics of Guatemala Colombian Colombian people',\n",
       " 'The predominant origin of regional Hispanic populations varies widely in different locations across the country . Hispanic Hispanic']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents[0:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['positive',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            query  \\\n",
      "0  Fox 2000 Pictures released the film Soul Food.   \n",
      "1  Fox 2000 Pictures released the film Soul Food.   \n",
      "2  Fox 2000 Pictures released the film Soul Food.   \n",
      "3  Fox 2000 Pictures released the film Soul Food.   \n",
      "4  Fox 2000 Pictures released the film Soul Food.   \n",
      "\n",
      "                                                text    labels  \n",
      "0  Soul Food is a 1997 American comedy-drama film...  positive  \n",
      "1  In 2015 , it was announced that 20th Century F...  negative  \n",
      "2  Featuring an ensemble cast , the film stars Va...  negative  \n",
      "3  Written and directed by George Tillman , Jr. -...  negative  \n",
      "4  Tillman based the family in the film on his ow...  negative  \n"
     ]
    }
   ],
   "source": [
    "df_data = {\"query\": queries, \"text\": sents, \"labels\": labels}\n",
    "df = pd.DataFrame(df_data)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "print(df.head())\n",
    "df.to_csv('all_to_all_interaction_dev.tsv', sep=\"\\t\",index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'queries_len' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m positive \u001b[39m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m negatives \u001b[39m=\u001b[39m []\n\u001b[0;32m----> 7\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(queries_len)):\n\u001b[1;32m      8\u001b[0m     \u001b[39mif\u001b[39;00m fever_nli[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m][i][\u001b[39m\"\u001b[39m\u001b[39mevidence_sentence_id\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n\u001b[1;32m      9\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'queries_len' is not defined"
     ]
    }
   ],
   "source": [
    "from random import sample \n",
    "import re\n",
    "\n",
    "queries = []\n",
    "positive = []\n",
    "negatives = []\n",
    "for i in tqdm(range(queries_len)):\n",
    "    if fever_nli[\"train\"][i][\"evidence_sentence_id\"] == -1:\n",
    "        continue\n",
    "    else:\n",
    "        try:\n",
    "            gold_doc_id = fever_nli[\"train\"][i]['evidence_wiki_url']\n",
    "            gold_sent_id = fever_nli[\"train\"][i]['evidence_sentence_id']\n",
    "            lines_original = wiki_id2lines[gold_doc_id].split(\"\\n\")\n",
    "            lines = []\n",
    "            for line in lines_original:\n",
    "                text = line.replace(\"\\t\",\" \").strip(\",. \")\n",
    "                text = re.sub(r'\\s+', ' ', text)\n",
    "                lines.append(text)\n",
    "        except:\n",
    "            continue\n",
    "        pos = lines[fever_nli[\"train\"][i]['evidence_sentence_id']]\n",
    "        # pos = re.sub(r'\\s+', ' ', pos)\n",
    "        lines = [lines[j] for j in range(len(lines)) if len(lines[j])>0 and lines[j].isnumeric()==False and j!=gold_sent_id]\n",
    "        # temp = [mapping_queries[fever_nli[\"train\"][i][\"claim\"]], mapping[pos]]\n",
    "        neg = sample(lines,min(len(lines), 8))\n",
    "\n",
    "        for m in range(0,min(len(lines), 8)):\n",
    "            queries.append(mapping_queries[fever_nli[\"train\"][i][\"claim\"]])\n",
    "            positive.append(mapping[pos])\n",
    "            negatives.append(mapping[neg[m]])\n",
    "        # for m in range(0,len(neg)):\n",
    "        #     negatives.append(neg[m])\n",
    "        # negatives += neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('fever_original_triples_1_pos_8_neg.json', 'w') as triples_file:\n",
    "    for i in range(0,len(negatives)):\n",
    "        temp = []\n",
    "        triples_file.write(json.dumps([queries[i], positives[i], negatives[i]])  + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16510508,\n",
       " 16510508,\n",
       " 16510508,\n",
       " 19750164,\n",
       " 19750164,\n",
       " 19750164,\n",
       " 19750166,\n",
       " 19750166,\n",
       " 19750166,\n",
       " 10318844,\n",
       " 10318844,\n",
       " 10318844]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16510505,\n",
       " 16510506,\n",
       " 16510510,\n",
       " 19750163,\n",
       " 19750167,\n",
       " 19750168,\n",
       " 19750165,\n",
       " 19750164,\n",
       " 19750163,\n",
       " 10318849,\n",
       " 10318848,\n",
       " 10318847]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 6611.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 75397, 'label': 'SUPPORTS', 'claim': 'Nikolaj Coster-Waldau worked with the Fox Broadcasting Company.', 'evidence_annotation_id': 92206, 'evidence_id': 104971, 'evidence_wiki_url': 'Nikolaj_Coster-Waldau', 'evidence_sentence_id': 7}\n",
      "-------------------------\n",
      "{'id': 150448, 'label': 'SUPPORTS', 'claim': 'Roman Atwood is a content creator.', 'evidence_annotation_id': 174271, 'evidence_id': 187498, 'evidence_wiki_url': 'Roman_Atwood', 'evidence_sentence_id': 1}\n",
      "-------------------------\n",
      "{'id': 150448, 'label': 'SUPPORTS', 'claim': 'Roman Atwood is a content creator.', 'evidence_annotation_id': 174271, 'evidence_id': 187499, 'evidence_wiki_url': 'Roman_Atwood', 'evidence_sentence_id': 3}\n",
      "-------------------------\n",
      "{'id': 214861, 'label': 'SUPPORTS', 'claim': 'History of art includes architecture, dance, sculpture, music, painting, poetry literature, theatre, narrative, film, photography and graphic arts.', 'evidence_annotation_id': 255136, 'evidence_id': 254645, 'evidence_wiki_url': 'History_of_art', 'evidence_sentence_id': 2}\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(10)):\n",
    "    if fever_nli[\"train\"][i][\"evidence_sentence_id\"] == -1:\n",
    "        continue\n",
    "    else:\n",
    "        print(fever_nli[\"train\"][i])\n",
    "        print(\"-------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 75397,\n",
       " 'label': 'SUPPORTS',\n",
       " 'claim': 'Nikolaj Coster-Waldau worked with the Fox Broadcasting Company.',\n",
       " 'evidence_annotation_id': 92206,\n",
       " 'evidence_id': 104971,\n",
       " 'evidence_wiki_url': 'Nikolaj_Coster-Waldau',\n",
       " 'evidence_sentence_id': 7}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fever_nli[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_line_corpus(pred_docs_ids):\n",
    "    line_corpus = []\n",
    "    wiki2lines = {}\n",
    "    for doc_id in pred_docs_ids:\n",
    "        try:\n",
    "            lines = wiki_id2lines[doc_id][1].split(\"\\n\")\n",
    "        except:\n",
    "            continue\n",
    "        for i in range(len(lines)):\n",
    "            text = lines[i].replace(\"\\t\",\" \").strip(\",. \")\n",
    "            text = re.sub(r'\\s+', ' ', text)\n",
    "            text = \" \".join(text.split()[1:])\n",
    "            wiki2lines[text] = [doc_id, i]\n",
    "            if text.isnumeric()==True or len(text)==0:\n",
    "                continue\n",
    "            line_corpus.append(text)\n",
    "    return(line_corpus, wiki2lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "import re\n",
    "\n",
    "queries = []\n",
    "bm25_lines = []\n",
    "for sample in tqdm(dev_data):\n",
    "    line_corpus, wiki2lines = return_line_corpus(retrieved_docs[sample['claim']])\n",
    "    tokenized_corpus = [doc.split(\" \") for doc in line_corpus]\n",
    "\n",
    "    bm25 = BM25Okapi(tokenized_corpus)\n",
    "    query = sample['claim']\n",
    "    tokenized_query = query.split(\" \")\n",
    "\n",
    "    retrieved_lines = bm25.get_top_n(tokenized_query, line_corpus, n=50)\n",
    "    for line in retrieved_lines:\n",
    "        queries.append(sample['claim'])\n",
    "        bm25_lines.append(wiki2lines[line])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bm25_lines)==len(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               query  \\\n",
      "0  Colin Kaepernick became a starting quarterback...   \n",
      "1  Colin Kaepernick became a starting quarterback...   \n",
      "2  Colin Kaepernick became a starting quarterback...   \n",
      "3  Colin Kaepernick became a starting quarterback...   \n",
      "4  Colin Kaepernick became a starting quarterback...   \n",
      "\n",
      "                                                text    labels  \n",
      "0  season was notable when Colin Kaepernick sat d...  negative  \n",
      "1  Wolf Pack also became the first team in colleg...  negative  \n",
      "2  49ers finished 30th in passing yards per game ...  negative  \n",
      "3  pistol has also made the transition to the NFL...  negative  \n",
      "4  to 2002 , the 49ers led the all-time series 4 ...  negative  \n"
     ]
    }
   ],
   "source": [
    "df_data = {\"query\": queries, \"text\": bm25_lines, \"labels\": [\"negative\" for i in range(len(bm25_lines))]}\n",
    "df = pd.DataFrame(df_data)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "print(df.head())\n",
    "df.to_csv('all_to_all_interaction_dev_bm25.tsv', sep=\"\\t\",index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
