{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FEVER: Fact Extraction and VERification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/nlp/sameer/anaconda3/envs/myenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/raid/nlp/sameer/anaconda3/envs/myenv/lib/python3.10/site-packages/datasets/load.py:1461: FutureWarning: The repository for fever contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/fever\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 5416537/5416537 [02:33<00:00, 35385.49it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from itertools import product\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import fever\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functionalities of Oracle Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "oracle = fever.Oracle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5416537/5416537 [02:42<00:00, 33424.89it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "fever_wiki = load_dataset(\"fever\", 'wiki_pages')\n",
    "\n",
    "wiki_id2lines = {}\n",
    "for dump in tqdm(fever_wiki['wikipedia_pages']):\n",
    "    if len(dump['text']) != 0:\n",
    "        wiki_id2lines[dump['id']] = [dump['text'], dump['lines']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "model = SentenceTransformer('/raid/nlp/sameer/fever/sbert_model/distilroberta')\n",
    "\n",
    "# Check if CUDA is available ans switch to GPU\n",
    "if torch.cuda.is_available():\n",
    "   model = model.to(torch.device(\"cuda:1\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End to End retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "fever_wiki = load_dataset(\"fever\", 'wiki_pages')\n",
    "# fever_nli = load_dataset(\"fever\", 'v1.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('/raid/nlp/sameer/fever/data/fever-data/train.jsonl') as f:\n",
    "    fever_nli = [json.loads(line) for line in f]\n",
    "\n",
    "with open('/raid/nlp/sameer/fever/data/fever-data/shared_task_test.jsonl') as f:\n",
    "    test_data = [json.loads(line) for line in f]\n",
    "\n",
    "with open('/raid/nlp/sameer/fever/data/fever-data/dev.jsonl') as f:\n",
    "    dev_data = [json.loads(line) for line in f]\n",
    "\n",
    "tfidf_level_1 = open('/raid/nlp/sameer/fever/tfidf_pred_level_1_test.json')\n",
    "level_1 = json.load(tfidf_level_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "model = SentenceTransformer('/raid/nlp/sameer/fever/sbert_model/distilroberta')\n",
    "\n",
    "# Check if CUDA is available ans switch to GPU\n",
    "if torch.cuda.is_available():\n",
    "   model = model.to(torch.device(\"cuda:1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 4457/9999 [09:20<11:04,  8.34it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 4470/9999 [09:22<10:21,  8.90it/s]"
     ]
    }
   ],
   "source": [
    "query2pred = {}\n",
    "invalid_id = []\n",
    "for sample in tqdm(dev_data):\n",
    "    try:\n",
    "        query = sample['claim']\n",
    "        # pred_sent = oracle.choose_sents_from_doc_ids(query, oracle.closest_docs(query, k=5), k=50)\n",
    "        pred_sent = oracle.choose_sents_from_doc_ids(query, level_1[query][:5], k=100)\n",
    "\n",
    "        query2pred[sample['id']] = [[i[0],i[1]] for i in list(pred_sent.keys())]\n",
    "    except:\n",
    "        invalid_id.append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "471"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(invalid_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_line_corpus(pred_docs_ids):\n",
    "    line_corpus = []\n",
    "    wiki2lines = {}\n",
    "    for doc_id in pred_docs_ids:\n",
    "        try:\n",
    "            lines = wiki_id2lines[doc_id][1].split(\"\\n\")\n",
    "        except:\n",
    "            continue\n",
    "        for i in range(len(lines)):\n",
    "            text = lines[i].replace(\"\\t\",\" \").strip(\",. \")\n",
    "            text = re.sub(r'\\s+', ' ', text)\n",
    "            text = \" \".join(text.split()[1:])\n",
    "            wiki2lines[text] = [doc_id, i]\n",
    "            if text.isnumeric()==True or len(text)==0:\n",
    "                continue\n",
    "            line_corpus.append(text)\n",
    "    return(line_corpus, wiki2lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19998/19998 [59:04<00:00,  5.64it/s]  \n"
     ]
    }
   ],
   "source": [
    "query2pred = {}\n",
    "\n",
    "from rank_bm25 import BM25Okapi\n",
    "import re\n",
    "# query2lines = {}\n",
    "\n",
    "for sample in tqdm(test_data):\n",
    "    line_corpus, wiki2lines = return_line_corpus(level_1[sample['claim']])\n",
    "    tokenized_corpus = [doc.split(\" \") for doc in line_corpus]\n",
    "\n",
    "    bm25 = BM25Okapi(tokenized_corpus)\n",
    "    query = sample['claim']\n",
    "    tokenized_query = query.split(\" \")\n",
    "\n",
    "    retrieved_lines = bm25.get_top_n(tokenized_query, line_corpus, n=50)\n",
    "    for line in retrieved_lines:\n",
    "        if sample['id'] not in query2pred:\n",
    "            query2pred[sample['id']] = [wiki2lines[line]]\n",
    "        else:\n",
    "            query2pred[sample['id']].append(wiki2lines[line])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19998\n"
     ]
    }
   ],
   "source": [
    "print(len(query2pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_tfidfboth_level_5_50.json\", \"w\") as outfile: \n",
    "    json.dump(query2pred, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89296,\n",
       " [['Dimitris', 0],\n",
       "  ['Yannis', 3],\n",
       "  ['List_of_alumni_of_Trinity_College,_Cambridge', 2],\n",
       "  ['List_of_Greek_exonyms_in_Turkey', 29],\n",
       "  ['List_of_Greek_exonyms_in_Turkey', 28],\n",
       "  ['List_of_Greek_exonyms_in_Turkey', 27],\n",
       "  ['List_of_Greek_exonyms_in_Turkey', 30],\n",
       "  ['List_of_Greek_exonyms_in_Turkey', 15],\n",
       "  ['List_of_Greek_exonyms_in_Turkey', 33],\n",
       "  ['List_of_Greek_exonyms_in_Turkey', 32],\n",
       "  ['List_of_Greek_exonyms_in_Turkey', 23],\n",
       "  ['List_of_Greek_exonyms_in_Turkey', 26],\n",
       "  ['List_of_Greek_exonyms_in_Turkey', 16],\n",
       "  ['List_of_Greek_exonyms_in_Turkey', 13],\n",
       "  ['List_of_Greek_exonyms_in_Turkey', 12],\n",
       "  ['List_of_Greek_exonyms_in_Turkey', 17],\n",
       "  ['List_of_Greek_exonyms_in_Turkey', 21],\n",
       "  ['List_of_Greek_exonyms_in_Turkey', 31],\n",
       "  ['List_of_Greek_exonyms_in_Turkey', 25],\n",
       "  ['List_of_Greek_exonyms_in_Turkey', 9],\n",
       "  ['List_of_Greek_exonyms_in_Turkey', 10],\n",
       "  ['List_of_Greek_exonyms_in_Turkey', 20],\n",
       "  ['List_of_Greek_exonyms_in_Turkey', 18],\n",
       "  ['List_of_alumni_of_Trinity_College,_Cambridge', 4],\n",
       "  ['List_of_Greek_exonyms_in_Turkey', 22],\n",
       "  ['Spencer,_Virginia', 0],\n",
       "  ['Spencer,_Virginia', 1],\n",
       "  ['Spencer,_Virginia', 5],\n",
       "  ['Spencer,_Virginia', 14],\n",
       "  ['List_of_alumni_of_Trinity_College,_Cambridge', 9],\n",
       "  ['List_of_alumni_of_Trinity_College,_Cambridge', 14],\n",
       "  ['List_of_Greek_exonyms_in_Turkey', 4],\n",
       "  ['List_of_Greek_exonyms_in_Turkey', 11],\n",
       "  ['Spencer,_Virginia', 2],\n",
       "  ['Spencer,_Virginia', 4],\n",
       "  ['Spencer,_Virginia', 11],\n",
       "  ['List_of_alumni_of_Trinity_College,_Cambridge', 3],\n",
       "  ['List_of_alumni_of_Trinity_College,_Cambridge', 12],\n",
       "  ['List_of_Greek_exonyms_in_Turkey', 0],\n",
       "  ['List_of_Greek_exonyms_in_Turkey', 1],\n",
       "  ['List_of_Greek_exonyms_in_Turkey', 2],\n",
       "  ['List_of_Greek_exonyms_in_Turkey', 3],\n",
       "  ['List_of_Greek_exonyms_in_Turkey', 5],\n",
       "  ['List_of_Greek_exonyms_in_Turkey', 6],\n",
       "  ['List_of_Greek_exonyms_in_Turkey', 7],\n",
       "  ['List_of_Greek_exonyms_in_Turkey', 8],\n",
       "  ['List_of_Greek_exonyms_in_Turkey', 14],\n",
       "  ['List_of_Greek_exonyms_in_Turkey', 19],\n",
       "  ['List_of_Greek_exonyms_in_Turkey', 24],\n",
       "  ['Yannis', 0]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(query2pred.items())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('/raid/nlp/sameer/fever/data/fever-data/dev.jsonl') as f:\n",
    "    data = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([131371, 146150, 'Telemundo', 4],\n",
       " 9999,\n",
       " {'id': 91198,\n",
       "  'verifiable': 'NOT VERIFIABLE',\n",
       "  'label': 'NOT ENOUGH INFO',\n",
       "  'claim': 'Colin Kaepernick became a starting quarterback during the 49ers 63rd season in the National Football League.',\n",
       "  'evidence': [[[108548, None, None, None]]]})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[4]['evidence'][2][0], len(data), data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "gold_docs = []\n",
    "id2gold_docs = {}\n",
    "id2gold_lines = {}\n",
    "for i in range(0,len(fever_nli)):\n",
    "    if fever_nli[i]['label'] == 'NOT ENOUGH INFO':\n",
    "        continue\n",
    "    gold = []\n",
    "    gold_line = []\n",
    "    for j in range(len(fever_nli[i]['evidence'])):\n",
    "        for k in range(len(fever_nli[i]['evidence'][j])):\n",
    "            if fever_nli[i]['evidence'][j][k][2]:\n",
    "                gold.append(fever_nli[i]['evidence'][j][k][2])\n",
    "            if fever_nli[i]['evidence'][j][k][3]:\n",
    "                try:\n",
    "                    text = wiki_id2lines[fever_nli[i]['evidence'][j][k][2]][1].split(\"\\n\")[fever_nli[i]['evidence'][j][k][3]]\n",
    "                    text = text.replace(\"\\t\", \" \")\n",
    "                    text = re.sub(r'\\s+', ' ', text)\n",
    "                    gold_line.append(text)\n",
    "                except:\n",
    "                    continue\n",
    "    if len(gold):\n",
    "        id2gold_docs[fever_nli[i]['claim']] = set(gold)\n",
    "    if len(gold_line):\n",
    "        id2gold_lines[fever_nli[i]['claim']] = set(gold_line)\n",
    "# gold_docs = [list(set(i)) for i in gold_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19998/19998 [55:37<00:00,  5.99it/s] \n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "valid_total = 0\n",
    "query2pred_docs = {}\n",
    "for i in tqdm(range(0,len(test_data))):\n",
    "    # if data[i]['label'] == 'NOT ENOUGH INFO':\n",
    "        query = test_data[i]['claim']\n",
    "    \n",
    "        # if query in id2gold_docs:\n",
    "            # valid_total += 1\n",
    "        pred_docs = oracle.closest_docs(query, k=100)\n",
    "        query2pred_docs[query] = pred_docs\n",
    "        # if len(set(pred_docs).intersection(id2gold_docs[query])):\n",
    "        #     cnt += 1\n",
    "# print(cnt/valid_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tfidf_pred_level_1_test.json\", \"w\") as outfile: \n",
    "    json.dump(query2pred_docs, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def retrieve(model, line_corpus, query, top_k):\n",
    "    pred_lines = []\n",
    "    corpus_embeddings = model.encode(line_corpus, convert_to_tensor=True).to(\"cuda:1\")\n",
    "    query_embedding = model.encode(query, convert_to_tensor=True).to(\"cuda:1\")\n",
    "    top_k = min(3, len(line_corpus))\n",
    "\n",
    "    # We use cosine-similarity and torch.topk to find the highest 5 scores\n",
    "    cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]\n",
    "    top_results = torch.topk(cos_scores, k=top_k)\n",
    "    for score, idx in zip(top_results[0], top_results[1]):\n",
    "        pred_lines.append(line_corpus[idx])\n",
    "    return(pred_lines)\n",
    "\n",
    "def return_line_corpus(pred_docs_ids):\n",
    "    line_corpus = []\n",
    "    wiki2lines = {}\n",
    "    for doc_id in pred_docs_ids:\n",
    "        try:\n",
    "            lines = wiki_id2lines[doc_id][1].split(\"\\n\")\n",
    "        except:\n",
    "            continue\n",
    "        for i in range(len(lines)):\n",
    "            text = lines[i].replace(\"\\t\",\" \").strip(\",. \")\n",
    "            text = re.sub(r'\\s+', ' ', text)\n",
    "            text = \" \".join(text.split()[1:])\n",
    "            wiki2lines[text] = [doc_id, i]\n",
    "            if text.isnumeric()==True or len(text)==0:\n",
    "                continue\n",
    "            line_corpus.append(text)\n",
    "    return(line_corpus, wiki2lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/145449 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'id2gold_docs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m,\u001b[39mlen\u001b[39m(fever_nli))):\n\u001b[1;32m      7\u001b[0m     query \u001b[39m=\u001b[39m data[i][\u001b[39m'\u001b[39m\u001b[39mclaim\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m----> 8\u001b[0m     \u001b[39mif\u001b[39;00m query \u001b[39min\u001b[39;00m id2gold_docs:\n\u001b[1;32m      9\u001b[0m         valid_total \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     10\u001b[0m         pred_docs_ids \u001b[39m=\u001b[39m oracle\u001b[39m.\u001b[39mclosest_docs(query, k\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'id2gold_docs' is not defined"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "cnt = 0\n",
    "valid_total = 0\n",
    "query2pred_lines = {}\n",
    "for i in tqdm(range(0,len(fever_nli))):\n",
    "    query = data[i]['claim']\n",
    "    if query in id2gold_docs:\n",
    "        valid_total += 1\n",
    "        pred_docs_ids = oracle.closest_docs(query, k=3)\n",
    "        line_corpus = return_line_corpus(pred_docs_ids)\n",
    "        if len(line_corpus) == 0:\n",
    "            continue\n",
    "        pred_lines = retrieve(model, line_corpus, query, 3)\n",
    "        query2pred_lines[query] = pred_lines\n",
    "        # for j in range(len(data[i]['evidence'])):\n",
    "        #     for k in range(len(data[i]['evidence'][j])):\n",
    "        #         if data[i]['evidence'][j][k][2]:\n",
    "        #             gold.append(data[i]['evidence'][j][k][3])\n",
    "        # if len(set(pred_docs).intersection(id2gold_docs[query])):\n",
    "        #     cnt += 1\n",
    "# print(cnt/valid_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SBert Zero shot first stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "model = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')\n",
    "\n",
    "# Check if CUDA is available ans switch to GPU\n",
    "if torch.cuda.is_available():\n",
    "   model = model.to(torch.device(\"cuda:1\"))\n",
    "print(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "with open('/raid/nlp/sameer/fever_old/embeddings.npy', 'rb') as f:\n",
    "    embeddings = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = fever_wiki['wikipedia_pages'][\"text\"][1:]\n",
    "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "\n",
    "# Step 3: Pass the index to IndexIDMap\n",
    "index = faiss.IndexIDMap(index)\n",
    "\n",
    "# Step 4: Add vectors and their IDs\n",
    "index.add_with_ids(x=embeddings[1:], ids=np.array([i for i in range(len(corpus))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D, I = index.search(np.array([query_embeddings[0]]), k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "valid_total = 0\n",
    "for i in tqdm(range(0,len(data))):\n",
    "    if data[i]['claim'] in id2gold_docs:\n",
    "        valid_total += 1\n",
    "        pred_docs = oracle.closest_docs(data[i]['claim'], k=10)\n",
    "        if len(set(pred_docs).intersection(id2gold_docs[data[i]['claim']])):\n",
    "            cnt += 1\n",
    "print(cnt/valid_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "colbert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "name": "_merged"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
